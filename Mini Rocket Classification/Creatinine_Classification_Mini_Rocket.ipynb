{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root = '/Volumes/SanDisk SSD/physionet.org/files/mimic-iv-ecg/1.0/'\n",
    "\n",
    "# split_csv = pd.read_csv(root + 'MIMIC-IV-ECG-Ext-Electrolytes/few_shot_splits/128shots/split1/Creatinine50912.csv')\n",
    "\n",
    "# labels = pd.read_csv(root + 'MIMIC-IV-ECG-Ext-Electrolytes/mimiciv_ECGv1.1_hospV2.2_Creatinine50912.csv')\n",
    "\n",
    "# train = []\n",
    "# test = []\n",
    "# val = []\n",
    "\n",
    "# for index, row in split_csv.iterrows():\n",
    "#     path = root + 'files/p' + f\"{row['subject_id']}\"[:4] + '/p' + f\"{row['subject_id']}\" + '/s' + f\"{row['study_id']}\"\n",
    "    \n",
    "#     signal, fields = wfdb.rdsamp(path + '/' + f\"{row['study_id']}\")\n",
    "\n",
    "#     if row['split'] == 'train':\n",
    "#         train.append(signal)\n",
    "#     elif row['split'] == 'test':\n",
    "#         test.append(signal)\n",
    "#     elif row['split'] == 'val':\n",
    "#         val.append(signal)\n",
    "\n",
    "# len(train), len(val), len(test)\n",
    "\n",
    "\n",
    "# faster version\n",
    "def process_row(row):\n",
    "    signal, fields = wfdb.rdsamp(row['path'] + '/' + str(row['study_id']))\n",
    "    return row['split'], signal\n",
    "\n",
    "root = '/Volumes/SanDisk SSD/physionet.org/files/mimic-iv-ecg/1.0/'\n",
    "\n",
    "# Load the CSV files\n",
    "split_csv = pd.read_csv(root + 'MIMIC-IV-ECG-Ext-Electrolytes/few_shot_splits/128shots/split1/Creatinine50912.csv')\n",
    "labels = pd.read_csv(root + 'MIMIC-IV-ECG-Ext-Electrolytes/mimiciv_ECGv1.1_hospV2.2_Creatinine50912.csv')\n",
    "\n",
    "# Precompute the paths\n",
    "split_csv['path'] = root + 'files/p' + split_csv['subject_id'].astype(str).str[:4] + '/p' + split_csv['subject_id'].astype(str) + '/s' + split_csv['study_id'].astype(str)\n",
    "\n",
    "# Initialize lists to store the signals\n",
    "train = []\n",
    "test = []\n",
    "val = []\n",
    "\n",
    "# Use pathos.multiprocessing to process rows in parallel\n",
    "with Pool() as pool:\n",
    "    results = pool.map(process_row, split_csv.to_dict('records'))\n",
    "\n",
    "# Organize the results into train, test, and val lists\n",
    "for split, signal in results:\n",
    "    if split == 'train':\n",
    "        train.append(signal)\n",
    "    elif split == 'test':\n",
    "        test.append(signal)\n",
    "    elif split == 'val':\n",
    "        val.append(signal)\n",
    "\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in X_train: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_train = np.empty((len(train), 12), dtype=object)\n",
    "\n",
    "for i in range(len(train)):\n",
    "    for j in range(12):\n",
    "        reshaped_train[i, j] = pd.Series(train[i][:][j]) # reshaping from (# of subjects, 5000, 12) to (# of subjects, 12) where each entry is a pd.series of length 5000\n",
    "\n",
    "X_train = pd.DataFrame(reshaped_train)\n",
    "\n",
    "null_counts = X_train.map(lambda x: x.isna().sum() if isinstance(x, pd.Series) else 0).sum().sum()\n",
    "\n",
    "X_train = X_train.map(lambda x: x.fillna(0) if isinstance(x, pd.Series) else x) # there were NaNs in the data, which is odd, so I'm filling them with 0\n",
    "print(f\"Number of null values in X_train: {null_counts}\")\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/3k_51w_j5yd8v5cpqy1brfqr0000gn/T/ipykernel_1089/3176496352.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_train = y_train.replace({'abnormal': 1, np.nan: 0}) # abnormal = 1, normal = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels = labels[labels['study_id'].isin(split_csv[split_csv['split'] == 'train']['study_id'])]\n",
    "y_train = filtered_labels['flag']\n",
    "y_train = y_train.replace({'abnormal': 1, np.nan: 0}) # abnormal = 1, normal = 0\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in X_val: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_val = np.empty((len(val), 12), dtype=object)\n",
    "for i in range(len(val)):\n",
    "    for j in range(12):\n",
    "        reshaped_val[i, j] = pd.Series(val[i][:][j])\n",
    "\n",
    "X_val = pd.DataFrame(reshaped_val)\n",
    "\n",
    "null_counts = X_val.map(lambda x: x.isna().sum() if isinstance(x, pd.Series) else 0).sum().sum()\n",
    "\n",
    "X_val = X_val.map(lambda x: x.fillna(0) if isinstance(x, pd.Series) else x) # there were NaNs in the data, which is odd, so I'm filling them with 0\n",
    "print(f\"Number of null values in X_val: {null_counts}\")\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/3k_51w_j5yd8v5cpqy1brfqr0000gn/T/ipykernel_1089/577769219.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_val = y_val.replace({'abnormal': 1, np.nan: 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels = labels[labels['study_id'].isin(split_csv[split_csv['split'] == 'val']['study_id'])]\n",
    "y_val = filtered_labels['flag']\n",
    "y_val = y_val.replace({'abnormal': 1, np.nan: 0})\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in X_test: 413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_test = np.empty((len(test), 12), dtype=object)\n",
    "for i in range(len(test)):\n",
    "    for j in range(12):\n",
    "        reshaped_test[i, j] = pd.Series(test[i][:][j])\n",
    "\n",
    "X_test = pd.DataFrame(reshaped_test)\n",
    "\n",
    "null_counts = X_test.map(lambda x: x.isna().sum() if isinstance(x, pd.Series) else 0).sum().sum()\n",
    "\n",
    "X_test = X_test.map(lambda x: x.fillna(0) if isinstance(x, pd.Series) else x) # there were NaNs in the data, which is odd, so I'm filling them with 0\n",
    "print(f\"Number of null values in X_test: {null_counts}\")\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/3k_51w_j5yd8v5cpqy1brfqr0000gn/T/ipykernel_1089/3775222411.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_test = y_test.replace({'abnormal': 1, np.nan: 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels = labels[labels['study_id'].isin(split_csv[split_csv['split'] == 'test']['study_id'])]\n",
    "y_test = filtered_labels['flag']\n",
    "y_test = y_test.replace({'abnormal': 1, np.nan: 0})\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernels: 1, Accuracy: 0.64844, F1 Score: 0.36620, AUROC: 0.56174\n",
      "Kernels: 5001, Accuracy: 0.65625, F1 Score: 0.46341, AUROC: 0.60410\n",
      "Kernels: 10001, Accuracy: 0.67188, F1 Score: 0.44737, AUROC: 0.60354\n",
      "Kernels: 15001, Accuracy: 0.66406, F1 Score: 0.41096, AUROC: 0.58555\n",
      "Kernels: 20001, Accuracy: 0.65625, F1 Score: 0.43590, AUROC: 0.59192\n",
      "Kernels: 25001, Accuracy: 0.63281, F1 Score: 0.31884, AUROC: 0.53793\n",
      "Kernels: 30001, Accuracy: 0.64062, F1 Score: 0.41026, AUROC: 0.57420\n",
      "Kernels: 35001, Accuracy: 0.62500, F1 Score: 0.41463, AUROC: 0.56866\n",
      "Kernels: 40001, Accuracy: 0.64062, F1 Score: 0.39474, AUROC: 0.56811\n",
      "Kernels: 45001, Accuracy: 0.70312, F1 Score: 0.50000, AUROC: 0.63898\n",
      "Kernels: 50001, Accuracy: 0.63281, F1 Score: 0.41975, AUROC: 0.57447\n",
      "Kernels: 55001, Accuracy: 0.64844, F1 Score: 0.40000, AUROC: 0.57392\n",
      "Kernels: 60001, Accuracy: 0.63281, F1 Score: 0.40506, AUROC: 0.56838\n",
      "Kernels: 65001, Accuracy: 0.63281, F1 Score: 0.37333, AUROC: 0.55620\n",
      "Kernels: 70001, Accuracy: 0.64062, F1 Score: 0.39474, AUROC: 0.56811\n",
      "Kernels: 75001, Accuracy: 0.64062, F1 Score: 0.39474, AUROC: 0.56811\n",
      "Kernels: 80001, Accuracy: 0.62500, F1 Score: 0.35135, AUROC: 0.54430\n",
      "Kernels: 85001, Accuracy: 0.64062, F1 Score: 0.45238, AUROC: 0.59247\n",
      "Kernels: 90001, Accuracy: 0.64844, F1 Score: 0.40000, AUROC: 0.57392\n",
      "Kernels: 95001, Accuracy: 0.68750, F1 Score: 0.48718, AUROC: 0.62735\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100000, 5000):\n",
    "    classifier = RocketClassifier(rocket_transform='minirocket', num_kernels=10000)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auroc = roc_auc_score(y_val, y_pred)\n",
    "    print(f\"Kernels: {i}, Accuracy: {accuracy:.5f}, F1 Score: {f1:.5f}, AUROC: {auroc:.5f}\")\n",
    "    \n",
    "    if i == 1:\n",
    "        scores_df = pd.DataFrame(columns=['Kernels', 'Accuracy', 'F1 Score', 'AUROC'])\n",
    "    \n",
    "    scores_df.loc[i] = [i, accuracy, f1, auroc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5919\n",
      "AUROC: 0.511401445987569\n",
      "F1 Score: 0.31145604859119286\n"
     ]
    }
   ],
   "source": [
    "classifier = RocketClassifier(rocket_transform='minirocket', num_kernels=45001)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUROC: {auroc}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
