{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.dists_kernels.dtw import DtwDtaidistMultiv\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root = '/Volumes/SanDisk SSD/physionet.org/files/mimic-iv-ecg/1.0/'\n",
    "\n",
    "# split_csv = pd.read_csv(root + 'MIMIC-IV-ECG-Ext-Electrolytes/few_shot_splits/128shots/split1/Calcium50893.csv')\n",
    "\n",
    "# labels = pd.read_csv(root + 'MIMIC-IV-ECG-Ext-Electrolytes/mimiciv_ECGv1.1_hospV2.2_Calcium50893.csv')\n",
    "\n",
    "# train = []\n",
    "# test = []\n",
    "# val = []\n",
    "\n",
    "# for index, row in split_csv.iterrows():\n",
    "#     path = root + 'files/p' + f\"{row['subject_id']}\"[:4] + '/p' + f\"{row['subject_id']}\" + '/s' + f\"{row['study_id']}\"\n",
    "    \n",
    "#     signal, fields = wfdb.rdsamp(path + '/' + f\"{row['study_id']}\")\n",
    "\n",
    "#     if row['split'] == 'train':\n",
    "#         train.append(signal)\n",
    "#     elif row['split'] == 'test':\n",
    "#         test.append(signal)\n",
    "#     elif row['split'] == 'val':\n",
    "#         val.append(signal)\n",
    "\n",
    "# len(train), len(val), len(test)\n",
    "\n",
    "\n",
    "# faster version\n",
    "def process_row(row):\n",
    "    signal, fields = wfdb.rdsamp(row['path'] + '/' + str(row['study_id']))\n",
    "    return row['split'], signal\n",
    "\n",
    "root = '/Volumes/SanDisk SSD/physionet.org/files/mimic-iv-ecg/1.0/'\n",
    "\n",
    "# Load the CSV files\n",
    "split_csv = pd.read_csv(root + 'MIMIC-IV-ECG-Ext-Electrolytes/few_shot_splits/128shots/split1/Calcium50893.csv')\n",
    "labels = pd.read_csv(root + 'MIMIC-IV-ECG-Ext-Electrolytes/mimiciv_ECGv1.1_hospV2.2_Calcium50893.csv')\n",
    "\n",
    "# Precompute the paths\n",
    "split_csv['path'] = root + 'files/p' + split_csv['subject_id'].astype(str).str[:4] + '/p' + split_csv['subject_id'].astype(str) + '/s' + split_csv['study_id'].astype(str)\n",
    "\n",
    "# Initialize lists to store the signals\n",
    "train = []\n",
    "test = []\n",
    "val = []\n",
    "\n",
    "# Use pathos.multiprocessing to process rows in parallel\n",
    "with Pool() as pool:\n",
    "    results = pool.map(process_row, split_csv.to_dict('records'))\n",
    "\n",
    "# Organize the results into train, test, and val lists\n",
    "for split, signal in results:\n",
    "    if split == 'train':\n",
    "        train.append(signal)\n",
    "    elif split == 'test':\n",
    "        test.append(signal)\n",
    "    elif split == 'val':\n",
    "        val.append(signal)\n",
    "\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in X_train: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_train = np.empty((len(train), 12), dtype=object)\n",
    "\n",
    "for i in range(len(train)):\n",
    "    for j in range(12):\n",
    "        reshaped_train[i, j] = pd.Series(train[i][:][j]) # reshaping from (# of subjects, 5000, 12) to (# of subjects, 12) where each entry is a pd.series of length 5000\n",
    "\n",
    "X_train = pd.DataFrame(reshaped_train)\n",
    "\n",
    "null_counts = X_train.map(lambda x: x.isna().sum() if isinstance(x, pd.Series) else 0).sum().sum()\n",
    "\n",
    "X_train = X_train.map(lambda x: x.fillna(0) if isinstance(x, pd.Series) else x) # there were NaNs in the data, which is odd, so I'm filling them with 0\n",
    "print(f\"Number of null values in X_train: {null_counts}\")\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/3k_51w_j5yd8v5cpqy1brfqr0000gn/T/ipykernel_91299/3176496352.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_train = y_train.replace({'abnormal': 1, np.nan: 0}) # abnormal = 1, normal = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels = labels[labels['study_id'].isin(split_csv[split_csv['split'] == 'train']['study_id'])]\n",
    "y_train = filtered_labels['flag']\n",
    "y_train = y_train.replace({'abnormal': 1, np.nan: 0}) # abnormal = 1, normal = 0\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in X_val: 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_val = np.empty((len(val), 12), dtype=object)\n",
    "for i in range(len(val)):\n",
    "    for j in range(12):\n",
    "        reshaped_val[i, j] = pd.Series(val[i][:][j])\n",
    "\n",
    "X_val = pd.DataFrame(reshaped_val)\n",
    "\n",
    "null_counts = X_val.map(lambda x: x.isna().sum() if isinstance(x, pd.Series) else 0).sum().sum()\n",
    "\n",
    "X_val = X_val.map(lambda x: x.fillna(0) if isinstance(x, pd.Series) else x) # there were NaNs in the data, which is odd, so I'm filling them with 0\n",
    "print(f\"Number of null values in X_val: {null_counts}\")\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/3k_51w_j5yd8v5cpqy1brfqr0000gn/T/ipykernel_91299/577769219.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_val = y_val.replace({'abnormal': 1, np.nan: 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels = labels[labels['study_id'].isin(split_csv[split_csv['split'] == 'val']['study_id'])]\n",
    "y_val = filtered_labels['flag']\n",
    "y_val = y_val.replace({'abnormal': 1, np.nan: 0})\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in X_test: 305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_test = np.empty((len(test), 12), dtype=object)\n",
    "for i in range(len(test)):\n",
    "    for j in range(12):\n",
    "        reshaped_test[i, j] = pd.Series(test[i][:][j])\n",
    "\n",
    "X_test = pd.DataFrame(reshaped_test)\n",
    "\n",
    "null_counts = X_test.map(lambda x: x.isna().sum() if isinstance(x, pd.Series) else 0).sum().sum()\n",
    "\n",
    "X_test = X_test.map(lambda x: x.fillna(0) if isinstance(x, pd.Series) else x) # there were NaNs in the data, which is odd, so I'm filling them with 0\n",
    "print(f\"Number of null values in X_test: {null_counts}\")\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/3k_51w_j5yd8v5cpqy1brfqr0000gn/T/ipykernel_91299/3775222411.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_test = y_test.replace({'abnormal': 1, np.nan: 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels = labels[labels['study_id'].isin(split_csv[split_csv['split'] == 'test']['study_id'])]\n",
    "y_test = filtered_labels['flag']\n",
    "y_test = y_test.replace({'abnormal': 1, np.nan: 0})\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbors: 1, Accuracy: 0.60938, F1 Score: 0.19355, AUROC: 0.47115\n",
      "Neighbors: 2, Accuracy: 0.76562, F1 Score: 0.00000, AUROC: 0.47115\n",
      "Neighbors: 3, Accuracy: 0.73438, F1 Score: 0.15000, AUROC: 0.50000\n",
      "Neighbors: 4, Accuracy: 0.79688, F1 Score: 0.07143, AUROC: 0.50641\n",
      "Neighbors: 5, Accuracy: 0.78125, F1 Score: 0.12500, AUROC: 0.51282\n",
      "Neighbors: 6, Accuracy: 0.78906, F1 Score: 0.00000, AUROC: 0.48558\n",
      "Neighbors: 7, Accuracy: 0.78125, F1 Score: 0.06667, AUROC: 0.49679\n",
      "Neighbors: 8, Accuracy: 0.80469, F1 Score: 0.00000, AUROC: 0.49519\n",
      "Neighbors: 9, Accuracy: 0.80469, F1 Score: 0.00000, AUROC: 0.49519\n",
      "Neighbors: 10, Accuracy: 0.80469, F1 Score: 0.00000, AUROC: 0.49519\n",
      "Neighbors: 11, Accuracy: 0.80469, F1 Score: 0.00000, AUROC: 0.49519\n",
      "Neighbors: 12, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 13, Accuracy: 0.80469, F1 Score: 0.00000, AUROC: 0.49519\n",
      "Neighbors: 14, Accuracy: 0.80469, F1 Score: 0.00000, AUROC: 0.49519\n",
      "Neighbors: 15, Accuracy: 0.80469, F1 Score: 0.00000, AUROC: 0.49519\n",
      "Neighbors: 16, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 17, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 18, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 19, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 20, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 21, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 22, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 23, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 24, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 25, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 26, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 27, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 28, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 29, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 30, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 31, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n",
      "Neighbors: 32, Accuracy: 0.81250, F1 Score: 0.00000, AUROC: 0.50000\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 33):\n",
    "    knn_classifier = KNeighborsTimeSeriesClassifier(n_neighbors=i, distance=DtwDtaidistMultiv())\n",
    "    knn_classifier.fit(X_train, y_train)\n",
    "    y_pred = knn_classifier.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auroc = roc_auc_score(y_val, y_pred)\n",
    "    print(f\"Neighbors: {i}, Accuracy: {accuracy:.5f}, F1 Score: {f1:.5f}, AUROC: {auroc:.5f}\")\n",
    "    \n",
    "    if i == 1:\n",
    "        scores_df = pd.DataFrame(columns=['Neighbors', 'Accuracy', 'F1 Score', 'AUROC'])\n",
    "    \n",
    "    scores_df.loc[i] = [i, accuracy, f1, auroc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7154\n",
      "AUROC: 0.5036228816248314\n",
      "F1 Score: 0.16096698113207547\n"
     ]
    }
   ],
   "source": [
    "knn_classifier = KNeighborsTimeSeriesClassifier(n_neighbors=3, distance=DtwDtaidistMultiv())\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "auroc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"AUROC: {auroc}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
